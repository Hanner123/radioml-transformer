# List experiment stages, i.e., the jobs to be run
stages:
  # Stage running model training
  train:
    # Stage runs the training script as the command
    #   Note: On Noctua 2 this requires a GPU node for training
    cmd: PARTITION=gpu bash run.sh python -m train
    # Data dependencies of this stage to determine when it needs to be rerun
    deps:
      # Run scripts orchestrating the script execution (potentially via sbatch)
      - run.sh
      - noctua.sh
      # The training and evaluation dataset
      - ${dataset.path}
      # The model, dataset and training python scripts
      - model.py
      - activations.py
      - dataset.py
      - train.py
    # Parameters used by this stage, changing any of these triggers reruns
    params:
      # Track the model, dataset and training hyperparameter sections from the
      # parameters file
      - model
      - dataset
      - train
      - seed
    # Outputs produced which should be tracked and passed on to the next stages
    outs:
      # The trained model checkpoint
      - outputs/model.pt
      # The optimizer state after training
      - outputs/optimizer.pt
      - outputs/urspruengliches_model.onnx
    # Plots produced by this stage
    plots:
      # Curves of training and validation loss per epoch
      - loss.yaml:
          # Explicitly plot the validation loss
          y: valid
          # Give a more readable title to the plot
          title: "Validation Loss per Epoch"
          # Track via git, not dvc cache
          cache: false
  # Stage running model evaluation after training to produce accuracy metrics
  eval:
    # Stage runs the evaluation script as the command
    cmd: python -m eval
    # Data dependencies of this stage to determine when it needs to be rerun
    deps:
      # The training and evaluation dataset
      - ${dataset.path}
      # The model checkpoint produced by the training stage
      - outputs/model.pt
      # The model, dataset and evaluation python scripts
      - model.py
      - activations.py
      - dataset.py
      - eval.py
    # Parameters used by this stage, changing any of these triggers reruns
    params:
      # Track the model, dataset and eval hyperparameter sections from the
      # parameters file
      - model
      - dataset
      - eval
      - seed
    # Metrics produced by this stage
    metrics:
      # Classification accuracy over the evaluation dataset
      - accuracy.yaml:
          # Track via git, not dvc cache
          cache: false
    # Plots produced by this stage
    plots:
      # Accuracy grouped by Signal-to-Noise Ratio
      - accuracy-per-snr.yaml:
          # Use the Signal-to_noise Ration levels as x-axis
          x: snr
          # Plot accuracy per SNR level as y-axis
          y: acc
          # Give a more readable title to the plot
          title: "Accuracy per SNR"
          # Track via git, not dvc cache
          cache: false
      # Confusion matrix of predicted vs. true classes
      - classes.csv:
          # Use true class label as x-axis
          x: cls
          # Use the predicted class label as y-axis
          y: prediction
          # Use the confusion matrix plot template
          template: confusion
          # Give a more readable title to the plot
          title: "Confusion Matrix"
          # Do not track via git as this file might be quite large, up to some
          # megabytes
          cache: true
  # Stage exporting the trained model to ONNX
  export:
    # Stage runs the export script as the command
    cmd: python -m export
    # Data dependencies of this stage to determine when it needs to be rerun
    deps:
      # The training and evaluation dataset
      - ${dataset.path}
      # The model checkpoint produced by the training stage
      - outputs/model.pt
      # The model, dataset and evaluation python scripts
      - model.py
      - activations.py
      - dataset.py
      - export.py
    # Parameters used by this stage, changing any of these triggers reruns
    params:
      # Track the model, dataset and export hyperparameter sections from the
      # parameters file
      - model
      - dataset
      - export
      - seed
    # Outputs produced which should be tracked and passed on to the next stages
    outs:
      # The exported model onnx graph file
      - outputs/model.onnx
      # Sample input-output pair for verification
      - outputs/inp.npy
      - outputs/out.npy
  # stage training and running a non-quantized model for speed up comparison
  nonquant:
    cmd: python -m export_untrained
    deps:
      - export_untrained.py
    outs:
      # The exported non-quantized model onnx graph file
      - outputs/model_nonquantized.onnx
  # stage measuring the speed of non-quantized model
  measure_32FP:
    cmd: FP16=0 python measure_untrained.py
    deps:
      - export_untrained.py
      - outputs/model_nonquantized.onnx
      - outputs/urspruengliches_model.onnx
      - measure_untrained.py
    plots:
     -  throughput/FP32/throughput_results.json:
         template: plot_templates/bar_2.json
         x: batch_size
         y: throughput_batches_per_s
         cache: false
     - throughput/FP32/throughput_results_2.json:
         template: plot_templates/bar_1.json
         x: batch_size
         y: throughput_images_per_s
         cache: false
     - throughput/FP32/latency_results.json:
         template: plot_templates/bar_3.json
         x: batch_size
         y: value
         cache: false
     - throughput/FP32/latency_results_batch.json:
         template: plot_templates/bar_4.json
         x: batch_size
         y: value
         cache: false
  measure_16FP:
    cmd: FP16=1 python measure_untrained.py
    deps:
      - export_untrained.py
      - outputs/model_nonquantized.onnx
      - measure_untrained.py
    plots:
     -  throughput/FP16/throughput_results.json:
         template: plot_templates/bar_2_FP16.json
         x: batch_size
         y: throughput_batches_per_s
         cache: false
     - throughput/FP16/throughput_results_2.json:
         template: plot_templates/bar_1_FP16.json
         x: batch_size
         y: throughput_images_per_s
         cache: false
     - throughput/FP16/latency_results.json:
         template: plot_templates/bar_3_FP16.json
         x: batch_size
         y: value
         cache: false
     - throughput/FP16/latency_results_batch.json:
         template: plot_templates/bar_4_FP16.json
         x: batch_size
         y: value
         cache: false
  quantize_tensorrt_INT8:
    cmd: python quantize_tensorrt.py 
    deps:
      - quantize_tensorrt.py
      - outputs/model_nonquantized.onnx
    outs:
      - outputs/engines
  measure_INT8_tensorrt:
    cmd: python measure_8bit.py 
    deps:
      - measure_8bit.py
      - plot_templates/bar_1_INT8_tensorrt.json
      - plot_templates/bar_2_INT8_tensorrt.json
      - plot_templates/bar_3_INT8_tensorrt.json
      - plot_templates/bar_4_INT8_tensorrt.json
      - outputs/engines
    outs:
      - eval_results/accuracy_INT8_tensorrt.json
    plots:
     -  throughput/INT8_tensorrt/throughput_results.json:
         template: plot_templates/bar_2_INT8_tensorrt.json
         x: batch_size
         y: throughput_batches_per_s
         cache: false
     - throughput/INT8_tensorrt/throughput_results_2.json:
         template: plot_templates/bar_1_INT8_tensorrt.json
         x: batch_size
         y: throughput_images_per_s
         cache: false
     - throughput/INT8_tensorrt/latency_results.json:
         template: plot_templates/bar_3_INT8_tensorrt.json
         x: batch_size
         y: value
         cache: false
     - throughput/INT8_tensorrt/latency_results_batch.json:
         template: plot_templates/bar_4_INT8_tensorrt.json
         x: batch_size
         y: value
         cache: false


  