# Model configuration section
model:
  # Number of attention heads
  num_heads: 8
  # Number of attention block "layers"
  num_layers: 1
  # Enables/disables bias on linear layers
  bias: True
  # Size of the input/output embedding dimension
  emb_dim: 64
  # Size of the MLP layer dimension
  mlp_dim: 256
  # Number of bits to use for quantized representation
  bits: 2
  # Number of classes at the classification head
  num_classes: 24
# Training/Validation dataset configuration
dataset:
  # Path to the dataset file
  path: "data/GOLD_XYZ_OSC.0001_1024.hdf5"
  # Optionally select only a subset of the classes
  #  classes: null
  # Optionally select only a subset of the available noise levels
  #  signal_to_noise_ratios: null
  # Splits the dataset into train/validation/evaluation subsets
  splits: [ 0.70, 0.15, 0.15 ]  # 70%, 15%, 15% splits
  # Seed used for reproducible splitting of datasets
  seed: 12
  # Reshape the dataset to fit the expected embedding dimension of the model
  # and to effectively reduce the sequence length
  reshape: [ -1, 64 ]  # -1 will infer the sequence length
# Training hyperparameters
train:
  # Batch size for training
  batch_size: 2048
  # Number of training epochs to run
  epochs: 3
  # Optimizer configuration
  optimizer:
    # Name of the optimization algorithm to use
    algorithm: "adam"
    # (Initial) Learning rate
    lr: 0.005
  # Loss function to use
  criterion: "cross-entropy"
  # DataLoader keyword arguments
  loader:
    # Reshuffle data every epoch
    shuffle: True
    # Number of workers to use for loading the data in parallel
    num_workers: 10
    # Number of batches loaded in advance by each worker
    prefetch_factor: 4
    # Keep worker processes alive after consuming the dataset once
    persistent_workers: True
    # Drop the last batch if it is incomplete, i.e., smaller than the batch size
    drop_last: True
