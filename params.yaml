# Global seed to make training and evaluation reproducible
# Note: Dataset splitting has its own seed
seed: 12
# Model configuration section
model:
  # Number of attention heads
  num_heads: 8
  # Number of attention block "layers"
  num_layers: 2
  # Enables/disables bias on linear layers
  bias: true
  # Size of the input/output embedding dimension
  emb_dim: 64
  # Size of the MLP layer dimension
  mlp_dim: 256
  # Type of normalization layer to use in the transformer blocks
  #   Options are: layer-norm, batch-norm and none
  norm: batch-norm
  # Dropout: probability of an element to be zeroed during training
  dropout: 0.0
  # Number of bits to use for quantized representation in "almost all layers"
  bits: 4
  # Quantization bit-width at the model inputs: Typically this should
  # be higher than for most other layers, e.g., keep this at 8 bits
  input_bits: 8
  # Quantization bit-width at the model outputs: Typically this should
  # be higher than for most other layers, e.g., keep this at 8 bits
  output_bits: 7
  # Number of classes at the classification head
  num_classes: 24
# Training/Validation dataset configuration
dataset:
  # Path to the dataset file
  path: data/GOLD_XYZ_OSC.0001_1024.hdf5
  # Optionally select only a subset of the classes
  #  classes: null
  # Optionally select only a subset of the available noise levels
  signal_to_noise_ratios: [
    # Noise levels from -6 dB upwards to 30 dB in steps of 2
    #   !!python/object/apply:builtins.range [-6, 31, 2]
    -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30
  ]
  # Splits the dataset into train/validation/evaluation subsets
  splits: [ 0.80, 0.10, 0.10 ]  # 80%, 10%, 10% splits
  # Seed used for reproducible splitting of datasets
  seed: 12
  # Reshape the dataset to fit the expected embedding dimension of the model
  # and to effectively reduce the sequence length
  reshape: [ -1, 64 ]  # -1 will infer the sequence length
# Training hyperparameters
train:
  # Batch size for training
  batch_size: 512
  # Number of training epochs to run
  epochs: 100
  # Optimizer configuration
  optimizer:
    # Name of the optimization algorithm to use
    algorithm: adam
    # (Initial) Learning rate
    lr: 0.001
    # L2 regularization
    weight_decay: 0.0
    # Coefficients used for computing running averages of gradient and its
    # square.
    # Note: Set according to Vaswani et al. 2017
    betas: [ 0.9, 0.98 ]
    # Term added to the denominator to improve numerical stability
    # Note: Set according to Vaswani et al. 2017
    eps: 1.0e-9
  # Loss function to use
  criterion: cross-entropy
  # DataLoader keyword arguments
  loader:
    # Reshuffle data every epoch
    shuffle: true
    # Number of workers to use for loading the data in parallel
    num_workers: 10
    # Number of batches loaded in advance by each worker
    prefetch_factor: 4
    # Keep worker processes alive after consuming the dataset once
    persistent_workers: true
    # Drop the last batch if it is incomplete, i.e., smaller than the batch size
    drop_last: true
# Evaluation hyperparameters
eval:
  # Evaluation batch size - does not influence the result, just speeds up the
  # evaluation by doing more in parallel
  batch_size: 4096
  # DataLoader keyword arguments
  loader:
    # Number of workers to use for loading the data in parallel
    num_workers: 10
    # Number of batches loaded in advance by each worker
    prefetch_factor: 4
    # Do not drop the last batch if it is incomplete
    drop_last: false
# Model to ONNX export hyperparameters
export:
  # Version of the default ONNX opset
  opset_version: 14
  # Apply the constant-folding optimization
  do_constant_folding: true
# FPGA dataflow accelerator build configuration
build:
  # FINN compiler configurations
  finn:
    # Directory to store the build outputs
    output_dir: outputs/build
    # Target clock period, i.e., inverse of target frequency
    synth_clk_period_ns: 10.0
    # Board to target with the build
    board: RFSoC2x2
    # Path to folding configuration JSON file
    folding_config_file: folding.json
    # Run synthesis to generate a .dcp for the stitched-IP output product
    stitched_ip_gen_dcp: True
    # Path to layer implementation style specialization config
    specialize_layers_config_file: specialize_layers.json
    # Force the implementation of standalone thresholds to be able to use RTL
    # implementation of the MVU
    # TODO: Cannot do this currently due to difficulties of applying the right
    # folding (in particular PE-parallelism for these)
    standalone_thresholds: False
    #  # Optional: Start the build from a specific step
    #  start_step: "step_tidy_up_pre_attention"
    #  # Optional: Stop the build after a specific step
    #  stop_step: "step_hls_ipgen"
  # Build metrics aggregation configuration
  metrics:
    # Path to the report file to be summarized
    # Note: remember to adjust the build directory when changing the config
    # above
    report: outputs/build/report/post_synth_resources.json
    # Filter the report rows
    filter: (top)
